import PIL
import requests
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline
import os 
from datasets import load_dataset
import torchvision.transforms as T
from PIL import Image, ImageDraw, ImageFont

from huggingface_hub import HfFolder
token = HfFolder.get_token()

model_id = "lukalafaye/luka"  # <- replace this
dataset_id = "lukalafaye/NoC"
N = 10  # Number of validation samples to process
output_dir = "inference_output"
os.makedirs(output_dir, exist_ok=True)

num_inference_steps = 200
image_guidance_scale = 3
guidance_scale = 10.0

pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16
).to("cuda")

generator = torch.Generator("cuda").manual_seed(0)
dataset = load_dataset(dataset_id, split="validation")
to_pil = T.ToPILImage()

# def download_image(url):
#    image = PIL.Image.open(requests.get(url, stream=True).raw)
#    image = PIL.ImageOps.exif_transpose(image)
#    image = image.convert("RGB")
#    return image


def save_side_by_side(input_image, ground_truth, pred_image, output_path, title=True):
    """
    Creates a side-by-side image showing the input, ground truth, and prediction,
    and saves it to the specified output path.

    Parameters:
        input_image (PIL.Image): The original input image.
        ground_truth (PIL.Image): The ground truth edited image.
        pred_image (PIL.Image): The image generated by the model.
        output_path (str): Path to save the combined image.
        title (bool): Whether to draw "Input", "Ground Truth", and "Prediction" labels.
    """
    spacer = 10
    w, h = input_image.size
    canvas = Image.new("RGB", (w * 3 + 2 * spacer, h), color=(255, 255, 255))

    canvas.paste(input_image, (0, 0))
    canvas.paste(ground_truth, (w + spacer, 0))
    canvas.paste(pred_image, (2 * w + 2 * spacer, 0))

    if title:
        draw = ImageDraw.Draw(canvas)
        try:
            font = ImageFont.truetype("arial.ttf", size=18)
        except:
            font = None  # Fallback to default font

        draw.text((w // 2 - 30, 5), "Input", font=font, fill=(0, 0, 0))
        draw.text((w + spacer + w // 2 - 50, 5), "Ground Truth", font=font, fill=(0, 0, 0))
        draw.text((2 * w + 2 * spacer + w // 2 - 40, 5), "Prediction", font=font, fill=(0, 0, 0))

    canvas.save(output_path)

for i in range(min(N, len(dataset))):
   sample = dataset[i]
   input_image = sample["input_image"]
   prompt = sample["edit_prompt"]
   ground_truth = sample["edited_image"]

   pred_image = pipe(
      prompt,
      image=input_image,
      num_inference_steps=num_inference_steps,
      image_guidance_scale=image_guidance_scale,
      guidance_scale=guidance_scale,
      generator=generator
    ).images[0]

   output_path = os.path.join(output_dir, f"val_{i:03d}.png")
   save_side_by_side(input_image, ground_truth, pred_image, output_path)
   print(f"[{i+1}/{N}] Saved to {output_path}")